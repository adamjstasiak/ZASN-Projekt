#  PatchConvNet with Multi-Head Latent Attention (MLA)


This repository contains an implementation of a hybrid model based on the **PatchConvNet** architecture, extended with a **Multi-Head Latent Attention (MLA)** mechanism. The aim of the project was to investigate the impact of the attentional feature proposed by the developers of the DeepSeek model, in an image classification task.


## ğŸ“Œ Key features

- ğŸ§  Custom implementation of **Multi-Head Latent Attention** with `latent queries`.
- âš™ï¸ Optional support for **Low-Rank Key/Value Compression**.
- ğŸ§ª Ablation tests: comparison of MLA with classical `Learned Aggregation`, GAP and GMP
- ğŸ“ˆ Selection of hyperparameters using **Optun** (run from a notebook)